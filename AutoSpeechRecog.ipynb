{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obCH8B0sxU7m"
      },
      "source": [
        "# HW3P2 - Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8sASSf7xaSc"
      },
      "source": [
        "# Instruction to Run the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqe6Cv_KuuSv"
      },
      "source": [
        "## To run the final model corresponding to the highest kaggle submission, please first make sure the **Global Variables** (next section) are set to fit the purpose, and then go to **Runtime**, click **Restart and run all**. This would\n",
        "1. pip install, import and download all required packages and data\n",
        "2. run all functions and classes for loading data and creating model/optimizer/scheduler\n",
        "3. train the model for 30 epochs based on the parameters saved in the config_king variable defined under **Parameter Configuration**.\n",
        "4. Sequentially finetune the model for 35 epoch with learning rates reset back to initial learning\n",
        "\n",
        "**Note** that by default, the notebook is expected to finish running in one click. If you pause the run and want to reload the model from saved path for finetuning, please set **RELOAD** as True under Global Variable section and under reload path in \"training setup\" section.\n",
        "\n",
        "**Note** that by default, the training data loads the 360 data. If you only want to run with 100, please set **USE100** as True under Global Variable section. The option to use combined dataset is not available for this homework (also not necessary).\n",
        "\n",
        "\n",
        "**Note** that by default, the notebook would run the trained model on the test dataset and save the predicted result in csv file, but it would not make the submission to Kaggle. To run the notebook with kaggle submission, set **SUBMIT_KAGGLE** to True."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yikr5hd8xnz7"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej4o-XUqWw6V"
      },
      "outputs": [],
      "source": [
        "REINSTALL = True # whether to reinstall packages and download datasets\n",
        "CONNECT_DRIVE = False # whether to connect to google drive\n",
        "USE_BASIC = False # whether to use basic network, only for early submission\n",
        "SUBMIT = False # whether to submit to kaggle\n",
        "RELOAD = False # whether to use reload model path, then enter reload\n",
        "USE100 = False # whether to use train-100 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66eNOJqy0anL"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "509ZChpH0gcm"
      },
      "source": [
        "## Best Score Hyperparameters:\n",
        "* **init learning rate** = 2e-3\n",
        "* **finetune init learning rate** = 2e-3\n",
        "* **beam width** = 30\n",
        "* **test beam width** = 150\n",
        "* **encoder embedding batchnorm**: after every Conv1D layer\n",
        "* **encoder embedding activate**: GELU\n",
        "* **encoder embedding dropout** = 0.25\n",
        "* **encoder LSTM dropout** = 0.25\n",
        "* **encoder pLSTM locked dropout** = 0.2\n",
        "* **encoder hidden size** = 256\n",
        "* **decoder batchnorm**: after every linear layer except for the last layer\n",
        "* **decoder activate**: GELU\n",
        "* **decoder dropout** = 0.35\n",
        "* **decoder hidden size** = 2048\n",
        "* **ASR embed size** = 256x2 (512)\n",
        "* **weight decay** = 5e-5\n",
        "* **batch size** = 128\n",
        "* **epoch** = 30\n",
        "* **finetune epoch** = 35\n",
        "* **weight init**: kaiming normal on conv1d and linear layer, constant on batchnorm layer\n",
        "* **scheduler**: ReduceLROnPlateau <patience = 1, factor = 0.5, threshold = 0.01, mode = min>\n",
        "* **optimizer**: AdamW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Data Loading Scheme:\n",
        "The highest kaggle score is run by loading and training on the train 360 dataset. The data loader that could load either 360 or 100 or validation are written in class **AudioDataset**, where all mfcc files are read in order and saved into a list for the final concatenation. No memory handling is used.\n",
        "\n",
        "All MFCCs data are normalized using Cepstral Normalization, and all Transcriptions data have EOS and SOS removed. No other data transform is used.\n",
        "\n",
        "Within the ASR model, input data from the loader went through frequency masking and time maksing, both with mask parameter = 10.\n",
        "\n",
        "\n",
        "## Architectures:\n",
        "The highest kaggle score is reached using ASR model with encoder and decoder.\n",
        "\n",
        "For encoder, the embedding include three blocks, between which a 0.25 dropout layer is added. The 1st block contains a conv1d, a batchnorm and a GELU activation layer. The kernel size used is 5. The 2nd block is a ResNet block with 256 in channels and 512 out channels. The ResNet block uses kernel size 3 for conv1d layer, and uses GELU activation. The 3rd block is a conv1d layer with kernel 3, 512 in channels and 512 out channels, and a batchnorm layer. No activation is added. In total, the embedding contains three convolutional blocks and 2 dropout layers.\n",
        "\n",
        "After the embedding, data are passed into a one-layer biLSTM with 0.25 dropout. The input size is the embed size (512) and output size is encoder hidden size (256).\n",
        "\n",
        "Then, the architecture includes a pBLSTMs block, which include two pBLSTM layer, after each with a LockedDropout layer with 0.2 dropout rate. The encoder hidden size (256) is used for pBLSTM output size.\n",
        "\n",
        "For decoder, the architecture is a 2 layer MLP with 2048 hidden size, followed by the final linear layer for output. Each non-final linear layer is followed by a batchnorm, a GELU activation, and a 0.35 dropout layer. At the end, LogSoftmax is applied on the final linear output of decoder.\n",
        "\n",
        "Other architectures are tested as well, but with less ideal performance:\n",
        "1. For encoder, a 3-layer pBLSTMs is attempted, but the sequences become too short for performance.\n",
        "2. For encoder, a 2-layer biLSTM + 2-layer pNLSTMs is attempted, but the performance does not improved.\n",
        "3. For encoder, simplier embedding without ResNet are attempted, and the performance is not as good as the complex embedding.\n",
        "4. For decoder, a 1 layer MLP is attempted.\n",
        "5. For decoder, a 4 layer MLP, each with 1024 hidden size is attempted.\n",
        "\n",
        "\n",
        "## Epochs\n",
        "I trained the model for 30 epochs and finetune for another 35 epochs to get to my highest kaggle submission. The model is close to converging at 30 epoch, and I reset the learning rate for finetuning again. At about 30 epoch, the model hit the high cutoff. I trained for another 5 epochs, but the model has not fully converged yet. I believe it has the potential to reach a even lower distance.\n",
        "\n",
        "\n",
        "## Hyperparameters\n",
        "* **beam width**: 30. I increased from 5 to 20 to 30, and find 30 a better width without sacrificing the runtime too much.\n",
        "\n",
        "* **test beam width**: 150. I increased from 50 to 100 to 150, and find the larger test width gives more accurate result, which is not too surprising.\n",
        "\n",
        "* **Batch Size**: 128. Given the computation capacity of my GCP, the maximum batch size I could use is 128 for the given model.\n",
        "\n",
        "* **Weight Decay**: 5e-5. I started with a higher weight decay based on previous homework, but during the ablation with a smaller weight, the model converge faster. Hence for this model, I lower the weight decay weight.\n",
        "\n",
        "* **Dropout**: on encoder, I attempted locked dropout rate from 0.1 to 0.5, and find 0.2 the best rate in terms of performance. On decoder, I used 0.35 based on HW1, and the result is quite pleasant, so I did not run ablation study on the rate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Other Experiments\n",
        "### Activation Function\n",
        "I selected the SiLU (for decoder) and ReLU6 (for encoder) activation function at first, based on HW1 and HW2, but it seems that GELU works slightly better in this homework. Hence I use GELU across both encoder and decoder.\n",
        "\n",
        "## Batchnorm\n",
        "I used batchnorm on every layer for encoder embedding and decoder MLP, except for the last layer. I did not attempt removing batchnorm in any layer.\n",
        "\n",
        "### Weight Initialization\n",
        "I used normal Kaiming initialization for conv1d and linear layer weight, and constant 1 vs 0 (weight vs bias) initialization for 1d batchnorm layer. I also tried Kaiming uniform initialization, and the performance is slightly worse.\n",
        "\n",
        "### Scheduler and Learning Rate\n",
        "I selected ReduceLROnPlateau with patience 1 and factor 0.5, with threshold 0.01 and min mode. I started with using patience = 3 and threshold as default, but the model converge very slowly and even when the model is not improving a lot, the learning rate stays unchanged. I then increase the default threshold 1e-4 to 0.01 reduce the patience, and the model converge much faster. I also attempted CosineAnnealWarmRestart schedule based on HW2, but it does not perform well in this task.\n",
        "\n",
        "\n",
        "### Optimizer\n",
        "I only switched between AdamW and Adam optimizer, and found the former better with weight decay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "if REINSTALL:\n",
        "  !pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "### Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7a7xeEoaV9"
      },
      "outputs": [],
      "source": [
        "if REINSTALL:\n",
        "  !pip install wandb --quiet\n",
        "  !pip install python-Levenshtein -q\n",
        "  !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "  !pip install wget -q\n",
        "  %cd ctcdecode\n",
        "  !pip install . -q\n",
        "  %cd ..\n",
        "\n",
        "  !pip install torchsummaryX -q\n",
        "  #!pip3 install torch==1.13.1 torchvision\n",
        "  #!pip3 install torchaudio==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "b0dc8d66-6a9f-4113-a8ee-6b3af9207b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdUelfGhom1m"
      },
      "outputs": [],
      "source": [
        "if REINSTALL:\n",
        "    !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "    !mkdir /root/.kaggle\n",
        "\n",
        "    with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "        f.write('{\"username\":\"sharonxin1207\",\"key\":\"xxx\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "    !chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSjBwfXeoq4B"
      },
      "outputs": [],
      "source": [
        "if REINSTALL:\n",
        "    !kaggle competitions download -c 11-785-s23-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ruxWP60LCQA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "if REINSTALL:\n",
        "    !unzip -q 11-785-s23-hw3p2.zip\n",
        "    !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "if CONNECT_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eN2kcxwXLLBb",
        "outputId": "5ce9ad20-0ecf-4418-e002-e0b7a3a1d67b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in ['B', 'IH', 'K', 'SH', 'AA']:\\n  print(CMUdict_ARPAbet[i])\\nprint(len(LABELS), len(PHONEMES))\\n\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You might want to play around with the mapping as a sanity check here\n",
        "\"\"\"\n",
        "for i in ['B', 'IH', 'K', 'SH', 'AA']:\n",
        "  print(CMUdict_ARPAbet[i])\n",
        "print(len(LABELS), len(PHONEMES))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root, partition='train-clean-100', transform=[\"norm\"]):\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = \"{}/{}/mfcc\".format(root, partition)\n",
        "        self.transcript_dir = \"{}/{}/transcript\".format(root, partition)\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "        self.PHONEMES = PHONEMES\n",
        "        self.mapping = {}\n",
        "        for p in range(len(self.PHONEMES)):\n",
        "          self.mapping[self.PHONEMES[p]] = p\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+self.mfcc_files[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            if \"norm\" in self.transform:\n",
        "                mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(self.transcript_dir+\"/\"+self.transcript_files[i])\n",
        "            start_idx = np.where(transcript=='[SOS]')[0][-1]\n",
        "            end_idx = np.where(transcript=='[EOS]')[0][0]\n",
        "            transcript = transcript[start_idx+1:end_idx]\n",
        "            transcript = np.array([self.mapping[t] for t in transcript])\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind])\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [b[0] for b in batch]\n",
        "        batch_transcript = [b[1] for b in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0) # TODO\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=0) # TODO\n",
        "        lengths_transcript = [len(trans) for trans in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n",
        "    def __init__(self, root, partition= \"test-clean\", transform=[\"norm\"]): # Feel free to add more arguments\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = \"{}/{}/mfcc\".format(root, partition)\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.mfccs = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+mfcc_names[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            if \"norm\" in self.transform:\n",
        "                mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "            self.mfccs.append(mfcc)\n",
        "        #self.mfccs          = np.concatenate(self.mfccs)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind]) # Convert to tensors\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        batch_mfcc_pad = pad_sequence(batch, batch_first=True, padding_value=0) # TODO\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128 # Increase if your device can handle it\n",
        "\n",
        "transforms = [\"norm\"] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = \"/content/11-785-s23-hw3p2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "b8a8f7ce-4bd4-4c82-fa88-0e954af02359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "d69dbd49-723d-46e6-bc63-dc50e33ea345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 104013, batches = 813\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "if USE100:\n",
        "  train_data = AudioDataset(root) #TODO\n",
        "else:\n",
        "  train_data = AudioDataset(root, 'train-clean-360') #TODO\n",
        "\n",
        "val_data = AudioDataset(root, 'dev-clean') # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "test_data = AudioDatasetTest(root) #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    collate_fn  = train_data.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    collate_fn  = val_data.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        "\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    collate_fn  = test_data.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "7e8a7c18-ad3b-46b3-9ce7-1db287c7f185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1702, 27]) torch.Size([128, 208]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "OUT_SIZE = len(LABELS)\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, embed_size=256):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        #self.embedding = torch.nn.Embedding(input_size, embed_size) # 27\n",
        "        self.conv1 = torch.nn.Conv1d(input_size, 128, kernel_size=3, stride=1, padding=1,bias=False)\n",
        "        self.batch1 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv2 = torch.nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1,bias=False)\n",
        "        self.batch2 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv3 = torch.nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1,bias=False)\n",
        "        self.batch3 = torch.nn.BatchNorm1d(512)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        hidden_size = 256\n",
        "        self.lstm = nn.LSTM(256, hidden_size = 256, num_layers = 1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.classification = nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size*2, hidden_size*2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(hidden_size*2, OUT_SIZE)\n",
        "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "        )\n",
        "\n",
        "\n",
        "        self.logSoftmax = torch.nn.LogSoftmax(dim=2)\n",
        "        #TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        #x = self.embedding(x)\n",
        "        embed_x = self.conv1(x.transpose(1,2))\n",
        "        embed_x = self.relu(self.batch1(embed_x))\n",
        "        embed_x = self.conv2(embed_x)\n",
        "        embed_x = self.relu(self.batch2(embed_x))\n",
        "        embed_x = self.conv3(embed_x)\n",
        "        embed_x = self.batch3(embed_x)\n",
        "        embed_x = embed_x.transpose(1,2)\n",
        "        print(embed_x.shape)\n",
        "        x_combined = pack_padded_sequence(embed_x, lx, batch_first=True, enforce_sorted=False)\n",
        "        out_combined, _ = self.lstm(x_combined)\n",
        "        out_x, out_lens = pad_packed_sequence(out_combined, batch_first=True)\n",
        "        out = self.logSoftmax(self.classification(out_x))\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        # Refer to the handout for hints\n",
        "        return out, out_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "## Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils Classes and Functions\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "# referring https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, xy):\n",
        "        x, y = xy\n",
        "        if not self.training or not self.dropout:\n",
        "            return pack_padded_sequence(x, y, batch_first=True, enforce_sorted=False)\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - self.dropout)\n",
        "        #mask = mask.to(device)\n",
        "        mask = mask.expand_as(x)\n",
        "        masked_x = pack_padded_sequence(mask * x, y, batch_first=True, enforce_sorted=False)\n",
        "        masked_x = masked_x.to(device)\n",
        "        return masked_x\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Conv1d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n",
        "    elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "        torch.nn.init.constant_(m.weight.data, 1)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input?\n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        self.blstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1,\n",
        "                                   batch_first=self.batch_first, bidirectional=True)\n",
        "        self.permute = PermuteBlock()\n",
        "        # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "        # TODO: Pad Packed Sequence\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        x_pad, x_pad_lens = pad_packed_sequence(x_packed, batch_first=self.batch_first)\n",
        "        x_trunc, x_trunc_lens = self.trunc_reshape(x_pad, x_pad_lens)\n",
        "        x_packed_pad_trunc = pack_padded_sequence(x_trunc, x_trunc_lens, batch_first=self.batch_first, enforce_sorted=False)\n",
        "        output, output_lens = self.blstm(x_packed_pad_trunc)\n",
        "        #output = output.to(device)\n",
        "        # What do you return?\n",
        "        output_pad, output_pad_lens = pad_packed_sequence(output, batch_first=self.batch_first)\n",
        "        # print(\"finishing padding\")\n",
        "        # output_pad = output_pad.to(device)\n",
        "        return output_pad, output_pad_lens\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "        # x = batch, seq_len, features\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        if x.shape[1] % 2!=0:\n",
        "            x = x[:, :-1, :]\n",
        "        x_down = x.contiguous().view(x.shape[0], x.shape[1] // 2, 2, x.shape[2])\n",
        "        x_down = torch.mean(x_down, 2)\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        x_lens = x_lens // 2\n",
        "        return x_down, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGUvJkckIIu-"
      },
      "source": [
        "### Building Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJXoJK_CF7Uj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConvBlock(torch.nn.Sequential):\n",
        "    def __init__(self, in_chan, out_chan, kernel, stride, padding=-1, groups=1, bias=False):\n",
        "        if padding < 0:\n",
        "          padding = (kernel-1)//2\n",
        "        super(ConvBlock, self).__init__(\n",
        "            torch.nn.Conv1d(in_channels=in_chan, out_channels=out_chan,\n",
        "                            kernel_size=kernel, stride=stride, padding=padding,\n",
        "                            groups=groups, bias=bias),\n",
        "            torch.nn.BatchNorm1d(out_chan),\n",
        "            #torch.nn.ReLU6(inplace=True)\n",
        "            torch.nn.GELU()\n",
        "        )\n",
        "\n",
        "class ResNetBlock(torch.nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        layer1 = ConvBlock(in_chan, out_chan, 3, self.stride)\n",
        "        layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(out_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(out_chan)\n",
        "        )\n",
        "        self.layerflat = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_chan, out_chan, kernel_size=1, stride=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(out_chan)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.basicnet = torch.nn.Sequential(*[layer1, layer2])\n",
        "        #self.activate = torch.nn.ReLU6(inplace=True)\n",
        "        self.activate = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.basicnet(x)\n",
        "        out += self.layerflat(x)\n",
        "        # to skip the connection if downsample condition is not met\n",
        "        out = self.activate(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-k1d7-fINfJ"
      },
      "source": [
        "### Encoder using ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdYZCWJWGNj9"
      },
      "outputs": [],
      "source": [
        "class ComplexEncoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, embed_size, encoder_hidden_size=256):\n",
        "        super(ComplexEncoder, self).__init__()\n",
        "        # construct layer for CNN embedding\n",
        "\n",
        "        layer1 = ConvBlock(input_size, 256, 5, 1)\n",
        "        layer2 = ResNetBlock(256, 512, 1)\n",
        "        layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(512, embed_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(embed_size)\n",
        "        )\n",
        "        layer_dropout = torch.nn.Dropout(0.25)\n",
        "\n",
        "        #TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            *[layer1, layer_dropout, layer2, layer_dropout, layer3])\n",
        "\n",
        "\n",
        "        self.permute = PermuteBlock()\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(embed_size, encoder_hidden_size, num_layers=1, dropout=0.25, batch_first=True, bidirectional=True)\n",
        "        self.pBLSTMs = torch.nn.Sequential(\n",
        "            #pBLSTM(encoder_hidden_size*2, encoder_hidden_size, True, True),\n",
        "            #LockedDropout(0.25),\n",
        "            pBLSTM(encoder_hidden_size*2, encoder_hidden_size, True),\n",
        "            LockedDropout(0.2),\n",
        "            pBLSTM(encoder_hidden_size*2, encoder_hidden_size, True),\n",
        "            LockedDropout(0.2)\n",
        "            # How many pBLSTMs are required?\n",
        "            # TODO: Fill this up with pBLSTMs - What should the input_size be?\n",
        "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission) ...\n",
        "            # ...\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        # note the x input should has been permuted\n",
        "        embed_x_t = self.embedding(x)\n",
        "        embed_x = self.permute(embed_x_t)\n",
        "        x_combined = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        lstm_output, _ = self.lstm(x_combined)\n",
        "        lstm_x, lstm_lens = pad_packed_sequence(lstm_output, batch_first=True)\n",
        "        x_combined = pack_padded_sequence(lstm_x, lstm_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        encoder_outputs = self.pBLSTMs(x_combined)\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        # TODO: Call the embedding layer\n",
        "        # TODO: Pack Padded Sequence\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        # TODO: Pad Packed Sequence\n",
        "\n",
        "        # Remember the number of output(s) each function returns\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oBkIhSIT0H"
      },
      "source": [
        "###Basic Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        # construct layer for CNN embedding\n",
        "        layer1 = ConvBlock(input_size, 128, 3, 1)\n",
        "        layer2 = ConvBlock(128, 256, 3, 1)\n",
        "        layer3 = ConvBlock(256, 256, 3, 1)\n",
        "        layer4 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(512)\n",
        "        )\n",
        "        #TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "        self.embedding = torch.nn.Sequential(*[layer1, layer2, layer3, layer4])\n",
        "        self.embedding_resnet = ResNet18(input_size)\n",
        "\n",
        "        self.permute = PermuteBlock()\n",
        "\n",
        "        #self.lstm = torch.nn.LSTM(512, 512, num_layers=2, dropout=0.25, batch_first=self.batch_first, bidirectional=True)\n",
        "        self.pBLSTMs = torch.nn.Sequential(\n",
        "            pBLSTM(encoder_hidden_size*2, encoder_hidden_size, True),\n",
        "            LockedDropout(0.25),\n",
        "            pBLSTM(encoder_hidden_size*2, encoder_hidden_size, True),\n",
        "            LockedDropout(0.25)\n",
        "            # How many pBLSTMs are required?\n",
        "            # TODO: Fill this up with pBLSTMs - What should the input_size be?\n",
        "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission) ...\n",
        "            # ...\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        # note the x input should has been permuted\n",
        "        embed_x_t = self.embedding(x)\n",
        "        embed_x = self.permute(embed_x_t)\n",
        "        x_combined = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "\n",
        "        lstm_output, _ = self.lstm(x_combined)\n",
        "        print(type(lstm_output))\n",
        "        lstm_x, lstm_lens = pad_packed_sequence(lstm_output, batch_first=True)\n",
        "        lstm_combined = pack_padded_sequence(lstm_x, lstm_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        #print(type(x_combined))\n",
        "        #lstm_output = self.lstm(x_combined)\n",
        "        encoder_outputs = self.pBLSTMs(lstm_combined)\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
        "\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embed_size, 2048),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(0.35),\n",
        "\n",
        "            torch.nn.Linear(2048, 2048),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(0.35),\n",
        "            torch.nn.Linear(2048, output_size),\n",
        "            #TODO define your MLP arch. Refer HW1P2\n",
        "            #Use Permute Block before and after BatchNorm1d() to match the size\n",
        "        )\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        #TODO call your MLP\n",
        "        decode_out = self.mlp(encoder_out)\n",
        "        #TODO Think what should be the final output of the decoder for the classification\n",
        "        out = self.softmax(decode_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBQty8GIIg-E"
      },
      "source": [
        "# ASR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augmentations  = torch.nn.Sequential(\n",
        "          tat.FrequencyMasking(freq_mask_param=10),\n",
        "          tat.TimeMasking(time_mask_param=10),\n",
        "          PermuteBlock()\n",
        "            #TODO Add Time Masking/ Frequency Masking\n",
        "\n",
        "            #Hint: See how to use PermuteBlock() function defined above\n",
        "        )\n",
        "        self.encoder        = ComplexEncoder(input_size, embed_size*2)# TODO: Initialize Encoder\n",
        "        self.decoder        = Decoder(embed_size*2, output_size)# TODO: Initialize Decoder\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "        else:\n",
        "            x = x.transpose(1, 2)\n",
        "\n",
        "        encoder_out, encoder_len   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT Basic\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "if USE_BASIC:\n",
        "  model = Network(27, 41).to(device)\n",
        "  summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## INIT ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "1e9eb0d6-739a-4c65-8ee8-a0391ee34e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): FrequencyMasking()\n",
            "    (1): TimeMasking()\n",
            "    (2): PermuteBlock()\n",
            "  )\n",
            "  (encoder): ComplexEncoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): ConvBlock(\n",
            "        (0): Conv1d(27, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.25, inplace=False)\n",
            "      (2): ResNetBlock(\n",
            "        (layerflat): Sequential(\n",
            "          (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (basicnet): Sequential(\n",
            "          (0): ConvBlock(\n",
            "            (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU6(inplace=True)\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activate): GELU(approximate='none')\n",
            "      )\n",
            "      (3): Dropout(p=0.25, inplace=False)\n",
            "      (4): Sequential(\n",
            "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (permute): PermuteBlock()\n",
            "    (lstm): LSTM(512, 256, batch_first=True, dropout=0.25, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "        (permute): PermuteBlock()\n",
            "      )\n",
            "      (1): LockedDropout()\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "        (permute): PermuteBlock()\n",
            "      )\n",
            "      (3): LockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (1): PermuteBlock()\n",
            "      (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): PermuteBlock()\n",
            "      (4): GELU(approximate='none')\n",
            "      (5): Dropout(p=0.35, inplace=False)\n",
            "      (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (7): PermuteBlock()\n",
            "      (8): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): PermuteBlock()\n",
            "      (10): GELU(approximate='none')\n",
            "      (11): Dropout(p=0.35, inplace=False)\n",
            "      (12): Linear(in_features=2048, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "=========================================================================================================\n",
            "                                                  Kernel Shape  \\\n",
            "Layer                                                            \n",
            "0_augmentations.FrequencyMasking_0                           -   \n",
            "1_augmentations.TimeMasking_1                                -   \n",
            "2_augmentations.PermuteBlock_2                               -   \n",
            "3_encoder.embedding.0.Conv1d_0                    [27, 256, 5]   \n",
            "4_encoder.embedding.0.BatchNorm1d_1                      [256]   \n",
            "5_encoder.embedding.0.ReLU6_2                                -   \n",
            "6_encoder.embedding.Dropout_1                                -   \n",
            "7_encoder.embedding.2.basicnet.0.Conv1d_0        [256, 512, 3]   \n",
            "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1           [512]   \n",
            "9_encoder.embedding.2.basicnet.0.ReLU6_2                     -   \n",
            "10_encoder.embedding.2.basicnet.1.Conv1d_0       [512, 512, 3]   \n",
            "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1          [512]   \n",
            "12_encoder.embedding.2.layerflat.Conv1d_0        [256, 512, 1]   \n",
            "13_encoder.embedding.2.layerflat.BatchNorm1d_1           [512]   \n",
            "14_encoder.embedding.2.GELU_activate                         -   \n",
            "15_encoder.embedding.Dropout_1                               -   \n",
            "16_encoder.embedding.4.Conv1d_0                  [512, 512, 3]   \n",
            "17_encoder.embedding.4.BatchNorm1d_1                     [512]   \n",
            "18_encoder.PermuteBlock_permute                              -   \n",
            "19_encoder.LSTM_lstm                                         -   \n",
            "20_encoder.pBLSTMs.0.LSTM_blstm                              -   \n",
            "21_encoder.pBLSTMs.LockedDropout_1                           -   \n",
            "22_encoder.pBLSTMs.2.LSTM_blstm                              -   \n",
            "23_encoder.pBLSTMs.LockedDropout_3                           -   \n",
            "24_decoder.mlp.Linear_0                            [512, 2048]   \n",
            "25_decoder.mlp.PermuteBlock_1                                -   \n",
            "26_decoder.mlp.BatchNorm1d_2                            [2048]   \n",
            "27_decoder.mlp.PermuteBlock_3                                -   \n",
            "28_decoder.mlp.GELU_4                                        -   \n",
            "29_decoder.mlp.Dropout_5                                     -   \n",
            "30_decoder.mlp.Linear_6                           [2048, 2048]   \n",
            "31_decoder.mlp.PermuteBlock_7                                -   \n",
            "32_decoder.mlp.BatchNorm1d_8                            [2048]   \n",
            "33_decoder.mlp.PermuteBlock_9                                -   \n",
            "34_decoder.mlp.GELU_10                                       -   \n",
            "35_decoder.mlp.Dropout_11                                    -   \n",
            "36_decoder.mlp.Linear_12                            [2048, 41]   \n",
            "37_decoder.LogSoftmax_softmax                                -   \n",
            "\n",
            "                                                     Output Shape     Params  \\\n",
            "Layer                                                                          \n",
            "0_augmentations.FrequencyMasking_0                [128, 1702, 27]          -   \n",
            "1_augmentations.TimeMasking_1                     [128, 1702, 27]          -   \n",
            "2_augmentations.PermuteBlock_2                    [128, 27, 1702]          -   \n",
            "3_encoder.embedding.0.Conv1d_0                   [128, 256, 1702]     34.56k   \n",
            "4_encoder.embedding.0.BatchNorm1d_1              [128, 256, 1702]      512.0   \n",
            "5_encoder.embedding.0.ReLU6_2                    [128, 256, 1702]          -   \n",
            "6_encoder.embedding.Dropout_1                    [128, 256, 1702]          -   \n",
            "7_encoder.embedding.2.basicnet.0.Conv1d_0        [128, 512, 1702]   393.216k   \n",
            "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1   [128, 512, 1702]     1.024k   \n",
            "9_encoder.embedding.2.basicnet.0.ReLU6_2         [128, 512, 1702]          -   \n",
            "10_encoder.embedding.2.basicnet.1.Conv1d_0       [128, 512, 1702]   786.432k   \n",
            "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1  [128, 512, 1702]     1.024k   \n",
            "12_encoder.embedding.2.layerflat.Conv1d_0        [128, 512, 1702]   131.072k   \n",
            "13_encoder.embedding.2.layerflat.BatchNorm1d_1   [128, 512, 1702]     1.024k   \n",
            "14_encoder.embedding.2.GELU_activate             [128, 512, 1702]          -   \n",
            "15_encoder.embedding.Dropout_1                   [128, 512, 1702]          -   \n",
            "16_encoder.embedding.4.Conv1d_0                  [128, 512, 1702]   786.432k   \n",
            "17_encoder.embedding.4.BatchNorm1d_1             [128, 512, 1702]     1.024k   \n",
            "18_encoder.PermuteBlock_permute                  [128, 1702, 512]          -   \n",
            "19_encoder.LSTM_lstm                                [161024, 512]   1.57696M   \n",
            "20_encoder.pBLSTMs.0.LSTM_blstm                      [80485, 512]   1.57696M   \n",
            "21_encoder.pBLSTMs.LockedDropout_1                   [80485, 512]          -   \n",
            "22_encoder.pBLSTMs.2.LSTM_blstm                      [40210, 512]   1.57696M   \n",
            "23_encoder.pBLSTMs.LockedDropout_3                   [40210, 512]          -   \n",
            "24_decoder.mlp.Linear_0                          [128, 425, 2048]  1.050624M   \n",
            "25_decoder.mlp.PermuteBlock_1                    [128, 2048, 425]          -   \n",
            "26_decoder.mlp.BatchNorm1d_2                     [128, 2048, 425]     4.096k   \n",
            "27_decoder.mlp.PermuteBlock_3                    [128, 425, 2048]          -   \n",
            "28_decoder.mlp.GELU_4                            [128, 425, 2048]          -   \n",
            "29_decoder.mlp.Dropout_5                         [128, 425, 2048]          -   \n",
            "30_decoder.mlp.Linear_6                          [128, 425, 2048]  4.196352M   \n",
            "31_decoder.mlp.PermuteBlock_7                    [128, 2048, 425]          -   \n",
            "32_decoder.mlp.BatchNorm1d_8                     [128, 2048, 425]     4.096k   \n",
            "33_decoder.mlp.PermuteBlock_9                    [128, 425, 2048]          -   \n",
            "34_decoder.mlp.GELU_10                           [128, 425, 2048]          -   \n",
            "35_decoder.mlp.Dropout_11                        [128, 425, 2048]          -   \n",
            "36_decoder.mlp.Linear_12                           [128, 425, 41]    84.009k   \n",
            "37_decoder.LogSoftmax_softmax                      [128, 425, 41]          -   \n",
            "\n",
            "                                                    Mult-Adds  \n",
            "Layer                                                          \n",
            "0_augmentations.FrequencyMasking_0                          -  \n",
            "1_augmentations.TimeMasking_1                               -  \n",
            "2_augmentations.PermuteBlock_2                              -  \n",
            "3_encoder.embedding.0.Conv1d_0                      58.82112M  \n",
            "4_encoder.embedding.0.BatchNorm1d_1                     256.0  \n",
            "5_encoder.embedding.0.ReLU6_2                               -  \n",
            "6_encoder.embedding.Dropout_1                               -  \n",
            "7_encoder.embedding.2.basicnet.0.Conv1d_0         669.253632M  \n",
            "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1          512.0  \n",
            "9_encoder.embedding.2.basicnet.0.ReLU6_2                    -  \n",
            "10_encoder.embedding.2.basicnet.1.Conv1d_0       1.338507264G  \n",
            "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1         512.0  \n",
            "12_encoder.embedding.2.layerflat.Conv1d_0         223.084544M  \n",
            "13_encoder.embedding.2.layerflat.BatchNorm1d_1          512.0  \n",
            "14_encoder.embedding.2.GELU_activate                        -  \n",
            "15_encoder.embedding.Dropout_1                              -  \n",
            "16_encoder.embedding.4.Conv1d_0                  1.338507264G  \n",
            "17_encoder.embedding.4.BatchNorm1d_1                    512.0  \n",
            "18_encoder.PermuteBlock_permute                             -  \n",
            "19_encoder.LSTM_lstm                                1.572864M  \n",
            "20_encoder.pBLSTMs.0.LSTM_blstm                     1.572864M  \n",
            "21_encoder.pBLSTMs.LockedDropout_1                          -  \n",
            "22_encoder.pBLSTMs.2.LSTM_blstm                     1.572864M  \n",
            "23_encoder.pBLSTMs.LockedDropout_3                          -  \n",
            "24_decoder.mlp.Linear_0                             1.048576M  \n",
            "25_decoder.mlp.PermuteBlock_1                               -  \n",
            "26_decoder.mlp.BatchNorm1d_2                           2.048k  \n",
            "27_decoder.mlp.PermuteBlock_3                               -  \n",
            "28_decoder.mlp.GELU_4                                       -  \n",
            "29_decoder.mlp.Dropout_5                                    -  \n",
            "30_decoder.mlp.Linear_6                             4.194304M  \n",
            "31_decoder.mlp.PermuteBlock_7                               -  \n",
            "32_decoder.mlp.BatchNorm1d_8                           2.048k  \n",
            "33_decoder.mlp.PermuteBlock_9                               -  \n",
            "34_decoder.mlp.GELU_10                                      -  \n",
            "35_decoder.mlp.Dropout_11                                   -  \n",
            "36_decoder.mlp.Linear_12                              83.968k  \n",
            "37_decoder.LogSoftmax_softmax                               -  \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "                            Totals\n",
            "Total params            12.206377M\n",
            "Trainable params        12.206377M\n",
            "Non-trainable params           0.0\n",
            "Mult-Adds             3.638225664G\n",
            "=========================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5c038a97-68e9-47d3-ab85-35938466d162\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_augmentations.FrequencyMasking_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1702, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_augmentations.TimeMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1702, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_augmentations.PermuteBlock_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 27, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.embedding.0.Conv1d_0</th>\n",
              "      <td>[27, 256, 5]</td>\n",
              "      <td>[128, 256, 1702]</td>\n",
              "      <td>34560.0</td>\n",
              "      <td>5.882112e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.embedding.0.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1702]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>2.560000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.embedding.0.ReLU6_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 256, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.embedding.Dropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 256, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.embedding.2.basicnet.0.Conv1d_0</th>\n",
              "      <td>[256, 512, 3]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>393216.0</td>\n",
              "      <td>6.692536e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.embedding.2.basicnet.0.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_encoder.embedding.2.basicnet.0.ReLU6_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_encoder.embedding.2.basicnet.1.Conv1d_0</th>\n",
              "      <td>[512, 512, 3]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>786432.0</td>\n",
              "      <td>1.338507e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_encoder.embedding.2.basicnet.1.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_encoder.embedding.2.layerflat.Conv1d_0</th>\n",
              "      <td>[256, 512, 1]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>131072.0</td>\n",
              "      <td>2.230845e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_encoder.embedding.2.layerflat.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_encoder.embedding.2.GELU_activate</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_encoder.embedding.Dropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_encoder.embedding.4.Conv1d_0</th>\n",
              "      <td>[512, 512, 3]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>786432.0</td>\n",
              "      <td>1.338507e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_encoder.embedding.4.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[128, 512, 1702]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_encoder.PermuteBlock_permute</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1702, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_encoder.LSTM_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[161024, 512]</td>\n",
              "      <td>1576960.0</td>\n",
              "      <td>1.572864e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[80485, 512]</td>\n",
              "      <td>1576960.0</td>\n",
              "      <td>1.572864e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_encoder.pBLSTMs.LockedDropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[80485, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_encoder.pBLSTMs.2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[40210, 512]</td>\n",
              "      <td>1576960.0</td>\n",
              "      <td>1.572864e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_encoder.pBLSTMs.LockedDropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[40210, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.Linear_0</th>\n",
              "      <td>[512, 2048]</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>1050624.0</td>\n",
              "      <td>1.048576e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.PermuteBlock_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.BatchNorm1d_2</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2.048000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.mlp.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_decoder.mlp.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_decoder.mlp.Dropout_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_decoder.mlp.Linear_6</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4.194304e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31_decoder.mlp.PermuteBlock_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32_decoder.mlp.BatchNorm1d_8</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[128, 2048, 425]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2.048000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33_decoder.mlp.PermuteBlock_9</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34_decoder.mlp.GELU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35_decoder.mlp.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36_decoder.mlp.Linear_12</th>\n",
              "      <td>[2048, 41]</td>\n",
              "      <td>[128, 425, 41]</td>\n",
              "      <td>84009.0</td>\n",
              "      <td>8.396800e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 425, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c038a97-68e9-47d3-ab85-35938466d162')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c038a97-68e9-47d3-ab85-35938466d162 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c038a97-68e9-47d3-ab85-35938466d162');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  Kernel Shape  \\\n",
              "Layer                                                            \n",
              "0_augmentations.FrequencyMasking_0                           -   \n",
              "1_augmentations.TimeMasking_1                                -   \n",
              "2_augmentations.PermuteBlock_2                               -   \n",
              "3_encoder.embedding.0.Conv1d_0                    [27, 256, 5]   \n",
              "4_encoder.embedding.0.BatchNorm1d_1                      [256]   \n",
              "5_encoder.embedding.0.ReLU6_2                                -   \n",
              "6_encoder.embedding.Dropout_1                                -   \n",
              "7_encoder.embedding.2.basicnet.0.Conv1d_0        [256, 512, 3]   \n",
              "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1           [512]   \n",
              "9_encoder.embedding.2.basicnet.0.ReLU6_2                     -   \n",
              "10_encoder.embedding.2.basicnet.1.Conv1d_0       [512, 512, 3]   \n",
              "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1          [512]   \n",
              "12_encoder.embedding.2.layerflat.Conv1d_0        [256, 512, 1]   \n",
              "13_encoder.embedding.2.layerflat.BatchNorm1d_1           [512]   \n",
              "14_encoder.embedding.2.GELU_activate                         -   \n",
              "15_encoder.embedding.Dropout_1                               -   \n",
              "16_encoder.embedding.4.Conv1d_0                  [512, 512, 3]   \n",
              "17_encoder.embedding.4.BatchNorm1d_1                     [512]   \n",
              "18_encoder.PermuteBlock_permute                              -   \n",
              "19_encoder.LSTM_lstm                                         -   \n",
              "20_encoder.pBLSTMs.0.LSTM_blstm                              -   \n",
              "21_encoder.pBLSTMs.LockedDropout_1                           -   \n",
              "22_encoder.pBLSTMs.2.LSTM_blstm                              -   \n",
              "23_encoder.pBLSTMs.LockedDropout_3                           -   \n",
              "24_decoder.mlp.Linear_0                            [512, 2048]   \n",
              "25_decoder.mlp.PermuteBlock_1                                -   \n",
              "26_decoder.mlp.BatchNorm1d_2                            [2048]   \n",
              "27_decoder.mlp.PermuteBlock_3                                -   \n",
              "28_decoder.mlp.GELU_4                                        -   \n",
              "29_decoder.mlp.Dropout_5                                     -   \n",
              "30_decoder.mlp.Linear_6                           [2048, 2048]   \n",
              "31_decoder.mlp.PermuteBlock_7                                -   \n",
              "32_decoder.mlp.BatchNorm1d_8                            [2048]   \n",
              "33_decoder.mlp.PermuteBlock_9                                -   \n",
              "34_decoder.mlp.GELU_10                                       -   \n",
              "35_decoder.mlp.Dropout_11                                    -   \n",
              "36_decoder.mlp.Linear_12                            [2048, 41]   \n",
              "37_decoder.LogSoftmax_softmax                                -   \n",
              "\n",
              "                                                     Output Shape     Params  \\\n",
              "Layer                                                                          \n",
              "0_augmentations.FrequencyMasking_0                [128, 1702, 27]        NaN   \n",
              "1_augmentations.TimeMasking_1                     [128, 1702, 27]        NaN   \n",
              "2_augmentations.PermuteBlock_2                    [128, 27, 1702]        NaN   \n",
              "3_encoder.embedding.0.Conv1d_0                   [128, 256, 1702]    34560.0   \n",
              "4_encoder.embedding.0.BatchNorm1d_1              [128, 256, 1702]      512.0   \n",
              "5_encoder.embedding.0.ReLU6_2                    [128, 256, 1702]        NaN   \n",
              "6_encoder.embedding.Dropout_1                    [128, 256, 1702]        NaN   \n",
              "7_encoder.embedding.2.basicnet.0.Conv1d_0        [128, 512, 1702]   393216.0   \n",
              "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1   [128, 512, 1702]     1024.0   \n",
              "9_encoder.embedding.2.basicnet.0.ReLU6_2         [128, 512, 1702]        NaN   \n",
              "10_encoder.embedding.2.basicnet.1.Conv1d_0       [128, 512, 1702]   786432.0   \n",
              "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1  [128, 512, 1702]     1024.0   \n",
              "12_encoder.embedding.2.layerflat.Conv1d_0        [128, 512, 1702]   131072.0   \n",
              "13_encoder.embedding.2.layerflat.BatchNorm1d_1   [128, 512, 1702]     1024.0   \n",
              "14_encoder.embedding.2.GELU_activate             [128, 512, 1702]        NaN   \n",
              "15_encoder.embedding.Dropout_1                   [128, 512, 1702]        NaN   \n",
              "16_encoder.embedding.4.Conv1d_0                  [128, 512, 1702]   786432.0   \n",
              "17_encoder.embedding.4.BatchNorm1d_1             [128, 512, 1702]     1024.0   \n",
              "18_encoder.PermuteBlock_permute                  [128, 1702, 512]        NaN   \n",
              "19_encoder.LSTM_lstm                                [161024, 512]  1576960.0   \n",
              "20_encoder.pBLSTMs.0.LSTM_blstm                      [80485, 512]  1576960.0   \n",
              "21_encoder.pBLSTMs.LockedDropout_1                   [80485, 512]        NaN   \n",
              "22_encoder.pBLSTMs.2.LSTM_blstm                      [40210, 512]  1576960.0   \n",
              "23_encoder.pBLSTMs.LockedDropout_3                   [40210, 512]        NaN   \n",
              "24_decoder.mlp.Linear_0                          [128, 425, 2048]  1050624.0   \n",
              "25_decoder.mlp.PermuteBlock_1                    [128, 2048, 425]        NaN   \n",
              "26_decoder.mlp.BatchNorm1d_2                     [128, 2048, 425]     4096.0   \n",
              "27_decoder.mlp.PermuteBlock_3                    [128, 425, 2048]        NaN   \n",
              "28_decoder.mlp.GELU_4                            [128, 425, 2048]        NaN   \n",
              "29_decoder.mlp.Dropout_5                         [128, 425, 2048]        NaN   \n",
              "30_decoder.mlp.Linear_6                          [128, 425, 2048]  4196352.0   \n",
              "31_decoder.mlp.PermuteBlock_7                    [128, 2048, 425]        NaN   \n",
              "32_decoder.mlp.BatchNorm1d_8                     [128, 2048, 425]     4096.0   \n",
              "33_decoder.mlp.PermuteBlock_9                    [128, 425, 2048]        NaN   \n",
              "34_decoder.mlp.GELU_10                           [128, 425, 2048]        NaN   \n",
              "35_decoder.mlp.Dropout_11                        [128, 425, 2048]        NaN   \n",
              "36_decoder.mlp.Linear_12                           [128, 425, 41]    84009.0   \n",
              "37_decoder.LogSoftmax_softmax                      [128, 425, 41]        NaN   \n",
              "\n",
              "                                                    Mult-Adds  \n",
              "Layer                                                          \n",
              "0_augmentations.FrequencyMasking_0                        NaN  \n",
              "1_augmentations.TimeMasking_1                             NaN  \n",
              "2_augmentations.PermuteBlock_2                            NaN  \n",
              "3_encoder.embedding.0.Conv1d_0                   5.882112e+07  \n",
              "4_encoder.embedding.0.BatchNorm1d_1              2.560000e+02  \n",
              "5_encoder.embedding.0.ReLU6_2                             NaN  \n",
              "6_encoder.embedding.Dropout_1                             NaN  \n",
              "7_encoder.embedding.2.basicnet.0.Conv1d_0        6.692536e+08  \n",
              "8_encoder.embedding.2.basicnet.0.BatchNorm1d_1   5.120000e+02  \n",
              "9_encoder.embedding.2.basicnet.0.ReLU6_2                  NaN  \n",
              "10_encoder.embedding.2.basicnet.1.Conv1d_0       1.338507e+09  \n",
              "11_encoder.embedding.2.basicnet.1.BatchNorm1d_1  5.120000e+02  \n",
              "12_encoder.embedding.2.layerflat.Conv1d_0        2.230845e+08  \n",
              "13_encoder.embedding.2.layerflat.BatchNorm1d_1   5.120000e+02  \n",
              "14_encoder.embedding.2.GELU_activate                      NaN  \n",
              "15_encoder.embedding.Dropout_1                            NaN  \n",
              "16_encoder.embedding.4.Conv1d_0                  1.338507e+09  \n",
              "17_encoder.embedding.4.BatchNorm1d_1             5.120000e+02  \n",
              "18_encoder.PermuteBlock_permute                           NaN  \n",
              "19_encoder.LSTM_lstm                             1.572864e+06  \n",
              "20_encoder.pBLSTMs.0.LSTM_blstm                  1.572864e+06  \n",
              "21_encoder.pBLSTMs.LockedDropout_1                        NaN  \n",
              "22_encoder.pBLSTMs.2.LSTM_blstm                  1.572864e+06  \n",
              "23_encoder.pBLSTMs.LockedDropout_3                        NaN  \n",
              "24_decoder.mlp.Linear_0                          1.048576e+06  \n",
              "25_decoder.mlp.PermuteBlock_1                             NaN  \n",
              "26_decoder.mlp.BatchNorm1d_2                     2.048000e+03  \n",
              "27_decoder.mlp.PermuteBlock_3                             NaN  \n",
              "28_decoder.mlp.GELU_4                                     NaN  \n",
              "29_decoder.mlp.Dropout_5                                  NaN  \n",
              "30_decoder.mlp.Linear_6                          4.194304e+06  \n",
              "31_decoder.mlp.PermuteBlock_7                             NaN  \n",
              "32_decoder.mlp.BatchNorm1d_8                     2.048000e+03  \n",
              "33_decoder.mlp.PermuteBlock_9                             NaN  \n",
              "34_decoder.mlp.GELU_10                                    NaN  \n",
              "35_decoder.mlp.Dropout_11                                 NaN  \n",
              "36_decoder.mlp.Linear_12                         8.396800e+04  \n",
              "37_decoder.LogSoftmax_softmax                             NaN  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "model = ASRModel(\n",
        "    input_size  = 27,\n",
        "    embed_size  = 256,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "model.apply(init_weights)\n",
        "summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 30,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 30,\n",
        "    \"finetune_epochs\": 35,\n",
        "    \"w_decay\": 5e-5,\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "criterion = torch.nn.CTCLoss() # Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['w_decay']) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(labels=LABELS, blank_id=0, beam_width=config['beam_width'], log_probs_input=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.01, verbose=True)\n",
        "#scheduler_cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 20, 1) #(optimizer, gamma=0.6, step_size=1)\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP=LABELS):\n",
        "\n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    (de_output, _, de_times, de_seq_lens) = decoder.decode(output, seq_lens=output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings                    = []\n",
        "\n",
        "    for i in range(output_lens.shape[0]):\n",
        "      if de_seq_lens[i, 0] != 0:\n",
        "        code = \"\".join(PHONEME_MAP[p] for p in de_output[i, 0, :de_seq_lens[i, 0]])\n",
        "      else:\n",
        "        code = \"\"\n",
        "      pred_strings.append(code)\n",
        "      #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "\n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    #label_strings   = [\"\".join(PHONEME_MAP[p] for p in l) for l in label]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i]\n",
        "        label_i = list(label[i][:label_lens[i]])\n",
        "        label_strings = [PHONEME_MAP[label_i[l]] for l in range(len(label_i))]\n",
        "        label_string = \"\".join(label_strings)\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    # raise NotImplemented\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "41ecc948-b7d5-4e46-898c-b75b52d123be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxinz3\u001b[0m (\u001b[33msharonxin1207\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " import wandb\n",
        "wandb.login(key=\"27ad915a9386068b1fc160cd97b84be7ba1fe659\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4s52yBOvICPZ",
        "outputId": "ea023dc4-3fe0-4d18-cb27-a56e3d012cab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230407_040201-1ladsttw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sharonxin1207/hw3p2-ablations/runs/1ladsttw' target=\"_blank\">run-resnet+plstm+lstm+30</a></strong> to <a href='https://wandb.ai/sharonxin1207/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sharonxin1207/hw3p2-ablations' target=\"_blank\">https://wandb.ai/sharonxin1207/hw3p2-ablations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sharonxin1207/hw3p2-ablations/runs/1ladsttw' target=\"_blank\">https://wandb.ai/sharonxin1207/hw3p2-ablations/runs/1ladsttw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "LOG_RUN = True\n",
        "if LOG_RUN:\n",
        "  run = wandb.init(\n",
        "      name = \"run-resnet+plstm+lstm\", ## Wandb creates random run names if you skip this field\n",
        "      reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "      # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "      # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "      project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
        "      config = config ### Wandb Config for your run\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "#torch.backends.cudnn.enabled = True\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        #with torch.autocast(device_type='cpu'):\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            h, lh = model(x, lx)\n",
        "            #print(\"running model done\")\n",
        "            h = torch.permute(h, (1, 0, 2))#.to(device)\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        if device == 'cpu':\n",
        "           loss.backward()\n",
        "           optimizer.step()\n",
        "        # Another couple things you need for FP16.\n",
        "        else:\n",
        "          scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "          scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "          scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"Finishing Train\")\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        #torch.cpu.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrEFE38MUz5"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "TEST_MODEL = False\n",
        "if TEST_MODEL:\n",
        "  model.eval()\n",
        "  for i, data in enumerate(val_loader, 0):\n",
        "      x, y, lx, ly = data\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      #x = x.transpose(1, 2) # use this step if not in basic mode\n",
        "      h, lh = model(x, lx)\n",
        "      print(h.shape)\n",
        "      print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "      h = torch.permute(h, (1, 0, 2))\n",
        "      print(h.shape, y.shape)\n",
        "      loss = criterion(h, y, lh, ly)\n",
        "      print(loss)\n",
        "\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tExvyl1BIdMC",
        "outputId": "a969eaba-b248-4990-9a45-dba59dbcd2df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "last_epoch_completed = 0\n",
        "START_EPO = last_epoch_completed\n",
        "END_EPO = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = \"epoch_model.pth\" #TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = \"best_model.pth\" #TODO set best model path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5ABirpQ7-sO"
      },
      "outputs": [],
      "source": [
        "if RELOAD:\n",
        "  reload_path = \"ENTER RELOAD PATH HERE\"\n",
        "  reload_items = load_model(reload_path, model, 'valid_dist', optimizer, scheduler)\n",
        "  model = items[0]\n",
        "  optimzer = items[1]\n",
        "\n",
        "  # updating above values if reloading from earlier model\n",
        "  best_lev_dist = items[4]\n",
        "  START_EPO = items[3]\n",
        "\n",
        "  print(best_lev_dist, optimizer.param_groups[0]['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygG4z_Qbyh4y"
      },
      "source": [
        "### For Init Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "outputs": [],
      "source": [
        "#TODO: Please complete the training loop\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(START_EPO, END_EPO):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer) #TODO\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder) #TODO\n",
        "    scheduler.step(valid_dist)\n",
        "    \"\"\"\n",
        "    if epoch < 39:\n",
        "      scheduler_cos.step()\n",
        "    elif epoch > 39:\n",
        "      scheduler.step(valid_dist)\n",
        "    else:\n",
        "      pass\n",
        "    \"\"\"\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "    \"\"\"\n",
        "    if epoch == 39:\n",
        "      optimizer.param_groups[0]['lr'] = 0.0002\n",
        "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.01, verbose=True)\n",
        "    \"\"\"\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUHXBsbykn1"
      },
      "source": [
        "### For Consequential Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLrifct1y3AW"
      },
      "outputs": [],
      "source": [
        "# reset learning rate\n",
        "optimizer.param_groups[0]['lr'] = config['lr']\n",
        "finetune_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n",
        "best_model_path = \"best_finetune_model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNirgbwkzh4h"
      },
      "outputs": [],
      "source": [
        "if LOG_RUN:\n",
        "  run = wandb.init(\n",
        "      name = \"run-resnet+plstm+lstm+finetune\", ## Wandb creates random run names if you skip this field\n",
        "      reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "      # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "      # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "      project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
        "      config = config ### Wandb Config for your run\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pEIvIJayowv"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(config['finetune_epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['finetune_epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer) #TODO\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder) #TODO\n",
        "    finetune_scheduler.step(valid_dist)\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "a18f311d-e48f-4194-b40f-a98f43132b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▍         | 1/21 [00:15<05:04, 15.22s/it]\u001b[A\n",
            " 10%|▉         | 2/21 [00:29<04:40, 14.77s/it]\u001b[A\n",
            " 14%|█▍        | 3/21 [00:42<04:06, 13.67s/it]\u001b[A\n",
            " 19%|█▉        | 4/21 [00:52<03:27, 12.22s/it]\u001b[A\n",
            " 24%|██▍       | 5/21 [01:06<03:29, 13.11s/it]\u001b[A\n",
            " 29%|██▊       | 6/21 [01:15<02:57, 11.80s/it]\u001b[A\n",
            " 33%|███▎      | 7/21 [01:26<02:38, 11.35s/it]\u001b[A\n",
            " 38%|███▊      | 8/21 [01:42<02:45, 12.70s/it]\u001b[A\n",
            " 43%|████▎     | 9/21 [01:57<02:43, 13.61s/it]\u001b[A\n",
            " 48%|████▊     | 10/21 [02:07<02:17, 12.51s/it]\u001b[A\n",
            " 52%|█████▏    | 11/21 [02:20<02:07, 12.71s/it]\u001b[A\n",
            " 57%|█████▋    | 12/21 [02:30<01:46, 11.85s/it]\u001b[A\n",
            " 62%|██████▏   | 13/21 [02:44<01:40, 12.52s/it]\u001b[A\n",
            " 67%|██████▋   | 14/21 [02:53<01:19, 11.29s/it]\u001b[A\n",
            " 71%|███████▏  | 15/21 [03:02<01:03, 10.64s/it]\u001b[A\n",
            " 76%|███████▌  | 16/21 [03:13<00:54, 10.88s/it]\u001b[A\n",
            " 81%|████████  | 17/21 [03:25<00:44, 11.20s/it]\u001b[A\n",
            " 86%|████████▌ | 18/21 [03:42<00:38, 12.75s/it]\u001b[A\n",
            " 90%|█████████ | 19/21 [03:56<00:26, 13.18s/it]\u001b[A\n",
            " 95%|█████████▌| 20/21 [04:07<00:12, 12.68s/it]\u001b[A\n",
            "100%|██████████| 21/21 [04:14<00:00, 12.11s/it]\n"
          ]
        }
      ],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 150\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(labels=LABELS, blank_id=0, beam_width=TEST_BEAM_WIDTH, log_probs_input=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string = decode_prediction(h, lh, test_decoder, LABELS)\n",
        "    #TODO save the output in results array.\n",
        "    results.append(prediction_string)\n",
        "\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70dvu_lsMlv",
        "outputId": "2d329344-f883-4013-aa0b-101f91881ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
            "100% 210k/210k [00:01<00:00, 190kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "if SUBMIT:\n",
        "  data_dir = root + \"/test-clean/random_submission.csv\"\n",
        "  df = pd.read_csv(data_dir)\n",
        "  concat_results = sum(results, [])\n",
        "  df.label = concat_results\n",
        "  df.to_csv('submission.csv', index = False)\n",
        "\n",
        "  !kaggle competitions submit -c 11-785-s23-hw3p2 -f submission.csv -m \"I made it!\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HLad4pChcuvX",
        "N2oBkIhSIT0H"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
